{"cells":[{"cell_type":"code","source":["%sh curl --remote-name-all 'https://files.training.databricks.com/assessments/cse-take-home/{covertype,kafka,treecover,u.data,u.item}.csv'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f44cb9c9-f01b-4606-a95d-3dc63ea7c8b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["\n[1/5]: https://files.training.databricks.com/assessments/cse-take-home/covertype.csv --> covertype.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   133  100   133    0     0   1528      0 --:--:-- --:--:-- --:--:--  1546\n\n[2/5]: https://files.training.databricks.com/assessments/cse-take-home/kafka.csv --> kafka.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  135k  100  135k    0     0  4110k      0 --:--:-- --:--:-- --:--:-- 4110k\n\n[3/5]: https://files.training.databricks.com/assessments/cse-take-home/treecover.csv --> treecover.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  642k  100  642k    0     0  17.9M      0 --:--:-- --:--:-- --:--:-- 17.9M\n\n[4/5]: https://files.training.databricks.com/assessments/cse-take-home/u.data.csv --> u.data.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 1932k  100 1932k    0     0  40.1M      0 --:--:-- --:--:-- --:--:-- 41.0M\n\n[5/5]: https://files.training.databricks.com/assessments/cse-take-home/u.item.csv --> u.item.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  230k  100  230k    0     0  9233k      0 --:--:-- --:--:-- --:--:-- 9233k\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/covertype.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/kafka.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/treecover.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.data.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.item.csv\n"]}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/databricks/driver/covertype.csv\", \"dbfs:/FileStore/tmp/covertype.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/kafka.csv\", \"dbfs:/FileStore/tmp/kafka.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/treecover.csv\", \"dbfs:/FileStore/tmp/treecover.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.data.csv\", \"dbfs:/FileStore/tmp/u.data.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.item.csv\", \"dbfs:/FileStore/tmp/u.item.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39883170-8b2a-49ad-93e3-c6b648b147d9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[14]: True"]}],"execution_count":0},{"cell_type":"code","source":["treeCoverDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tmp/treecover.csv\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc9b56c9-9e47-436e-91ab-4a3137162ba7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["treeCoverDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8cd5451c-88b5-455e-beab-28f5ceb3e8c0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Id: integer (nullable = true)\n |-- Elevation: integer (nullable = true)\n |-- Aspect: integer (nullable = true)\n |-- Slope: integer (nullable = true)\n |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n |-- Cover_Type: integer (nullable = true)\n |-- Soil_Type: integer (nullable = true)\n |-- Wilderness_Area: integer (nullable = true)\n |-- Hillshade: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["num_rows = treeCoverDF.count()\nnum_cols = len(treeCoverDF.columns)\n\nprint(\"Number of rows: {}\".format(num_rows))\nprint(\"Number of columns: {}\".format(num_cols))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ab444f6-de6a-40be-99ef-650694d5fbed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Number of rows: 15120\nNumber of columns: 12\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nelevation_stats = treeCoverDF.select(col(\"elevation\")).describe().toPandas()\nelevation_range = (elevation_stats.loc[3, \"elevation\"], elevation_stats.loc[4, \"elevation\"])\nelevation_mean = float(elevation_stats.loc[1, \"elevation\"])\nelevation_stddev = float(elevation_stats.loc[2, \"elevation\"])\n\nprint(\"Range of elevation: {}\".format(elevation_range))\nprint(\"Mean elevation: {}\".format(elevation_mean))\nprint(\"Standard deviation of elevation: {}\".format(elevation_stddev))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"360b4f3b-5f5b-4015-8427-0665b38b06f4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Range of elevation: ('1863', '3849')\nMean elevation: 2749.3225529100528\nStandard deviation of elevation: 417.67818734804985\n"]}],"execution_count":0},{"cell_type":"code","source":["num_entries = treeCoverDF.filter((col(\"elevation\") >= 2749.32) & ((col(\"Cover_Type\") == 1) | (col(\"Cover_Type\") == 2))).count()\n\nprint(\"Number of entries with elevation >= 2749.32 meters and Cover_Type 1 or 2: {}\".format(num_entries))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3bc7700d-1f8c-43d1-9d32-79aa019cb392","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Number of entries with elevation >= 2749.32 meters and Cover_Type 1 or 2: 3883\n"]}],"execution_count":0},{"cell_type":"code","source":["num_matching_entries = treeCoverDF.filter((col(\"elevation\") >= 2749.32) & ((col(\"Cover_Type\") == 1) | (col(\"Cover_Type\") == 2))).count()\nnum_cover_type_entries = treeCoverDF.filter((col(\"Cover_Type\") == 1) | (col(\"Cover_Type\") == 2)).count()\n\npercent_matching_entries = 100.0 * num_matching_entries / num_cover_type_entries\n\nprint(\"Percentage of entries with Cover_Type 1 or 2 and elevation at or above 2749.32 meters: {:.2f}%\".format(percent_matching_entries))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48745597-c335-407b-87e8-817a244db431","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Percentage of entries with Cover_Type 1 or 2 and elevation at or above 2749.32 meters: 89.88%\n"]}],"execution_count":0},{"cell_type":"code","source":["display(treeCoverDF.groupBy(\"Wilderness_Area\").count())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2a3db5d-bb93-4d21-887d-7b1ac99f7cec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,3597],[3,6349],[4,4675],[2,499]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Wilderness_Area","type":"\"integer\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Wilderness_Area</th><th>count</th></tr></thead><tbody><tr><td>1</td><td>3597</td></tr><tr><td>3</td><td>6349</td></tr><tr><td>4</td><td>4675</td></tr><tr><td>2</td><td>499</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Read in the covertype.csv file\ncover_type_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/tmp/covertype.csv\")\n\n# Join the two DataFrames based on the Cover_Type column\njoined_df = treeCoverDF.join(cover_type_df, on=\"outer\")\n\n# Group the DataFrame by the Cover Type Labels and calculate the average elevation\navg_elevation_df = joined_df.groupBy(\"Cover_Type_Label\").avg(\"elevation\").orderBy(\"Cover_Type_Label\")\n\n# Pass the resulting DataFrame to the display function to generate a bar chart visualization\ndisplay(avg_elevation_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf6c419b-b429-42b4-84ba-933b10f1451f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-295471767776684>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Join the two DataFrames based on the Cover_Type column\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mjoined_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtreeCoverDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcover_type_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"outer\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Group the DataFrame by the Cover Type Labels and calculate the average elevation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   1623\u001B[0m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1624\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"how should be a string\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1625\u001B[0;31m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1626\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: USING column `outer` cannot be resolved on the left side of the join. The left-side columns: [Id, Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Fire_Points, Cover_Type, Soil_Type, Wilderness_Area, Hillshade]","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: USING column `outer` cannot be resolved on the left side of the join. The left-side columns: [Id, Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Fire_Points, Cover_Type, Soil_Type, Wilderness_Area, Hillshade]","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-295471767776684>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Join the two DataFrames based on the Cover_Type column\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mjoined_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtreeCoverDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcover_type_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"outer\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Group the DataFrame by the Cover Type Labels and calculate the average elevation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   1623\u001B[0m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1624\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"how should be a string\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1625\u001B[0;31m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1626\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: USING column `outer` cannot be resolved on the left side of the join. The left-side columns: [Id, Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Fire_Points, Cover_Type, Soil_Type, Wilderness_Area, Hillshade]"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n\n# Define the schema for the u.data.csv file\nu_data_schema = StructType([\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"item_id\", IntegerType(), True),\n    StructField(\"rating\", IntegerType(), True),\n    StructField(\"timestamp\", IntegerType(), True)\n])\n\n# Define the schema for the u.item.csv file\nu_item_schema = StructType([\n    StructField(\"item_id\", IntegerType(), True),\n    StructField(\"movie_title\", StringType(), True),\n    StructField(\"release_date\", StringType(), True),\n    StructField(\"video_release_date\", StringType(), True),\n    StructField(\"IMDb_URL\", StringType(), True),\n    StructField(\"unknown\", IntegerType(), True),\n    StructField(\"Action\", IntegerType(), True),\n    StructField(\"Adventure\", IntegerType(), True),\n    StructField(\"Animation\", IntegerType(), True),\n    StructField(\"Childrens\", IntegerType(), True),\n    StructField(\"Comedy\", IntegerType(), True),\n    StructField(\"Crime\", IntegerType(), True),\n    StructField(\"Documentary\", IntegerType(), True),\n    StructField(\"Drama\", IntegerType(), True),\n    StructField(\"Fantasy\", IntegerType(), True),\n    StructField(\"Film_Noir\", IntegerType(), True),\n    StructField(\"Horror\", IntegerType(), True),\n    StructField(\"Musical\", IntegerType(), True),\n    StructField(\"Mystery\", IntegerType(), True),\n    StructField(\"Romance\", IntegerType(), True),\n    StructField(\"Sci_Fi\", IntegerType(), True),\n    StructField(\"Thriller\", IntegerType(), True),\n    StructField(\"War\", IntegerType(), True),\n    StructField(\"Western\", IntegerType(), True)\n])\n\n# Read in the u.data.csv file using the specified schema and load it into a DataFrame\nuDataDF = spark.read.format(\"csv\").schema(u_data_schema).load(\"dbfs:/FileStore/tmp/u.data.csv\")\n\n# Read in the u.item.csv file using the specified schema and load it into a DataFrame, dropping unnecessary columns\nuItemDF = spark.read.format(\"csv\").schema(u_item_schema).load(\"dbfs:/FileStore/tmp/u.item.csv\").drop(\"movie_title\", \"video_release_date\", \"IMDb_URL\")\n\n# Convert the release_date column to a DateType with the format yyyy-MM-dd\nuItemDF = uItemDF.withColumn(\"release_date\", uItemDF[\"release_date\"].cast(StringType()))\nuItemDF = uItemDF.withColumn(\"release_date\", uItemDF[\"release_date\"].cast(DateType()))\n\n# Order the uDataDF DataFrame by the date column\nuDataDF = uDataDF.orderBy(\"timestamp\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"692633ac-867b-4081-acdf-a3f3bff33173","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#recommendations can be made:\n\n#Use IAM roles and policies to manage access to Databricks resources.\n#Encrypt data at rest using AWS Key Management Service (KMS) and enable encryption in transit using SSL/TLS.\n#Use network security best practices such as Virtual Private Cloud (VPC) and Security Groups to restrict access to Databricks clusters and workspaces.#"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35697eb4-a090-4b60-934b-e39765b983da","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#  In 4-5 sentences, please explain what the Databricks API is used for at a high-level.\n\n\n # The Databricks API is a set of RESTful APIs that allows developers and data scientists to programmatically manage and interact with Databricks workspaces, clusters, jobs, and data. It can be used to automate common tasks such as creating and deleting clusters, running and monitoring jobs, uploading and downloading data, and managing user access and permissions. The API provides a convenient way to integrate Databricks with other tools and services and enables users to build custom applications and workflows that leverage the power of Databricks."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21172482-34c9-4857-803a-e1a0ea7b37fe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Question 14: Conceptual Question - Explain an API Call\n#In 4-5 sentences, please explain what this API call. Be sure to discuss some key attributes about the cluster\n\n#The API call typically includes a set of parameters and an HTTP request method such as GET, POST, PUT, or DELETE. In the context of Databricks, an API call might be used to create, delete, or modify a cluster, submit a job, or retrieve data from a Databricks workspace. Some key attributes of a Databricks cluster that might be included in an API call include the number and type of nodes, the version of Spark being used, the cluster name, and any additional configuration settings such as auto-scaling and auto-termination policies."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"711f6ada-f248-418c-88e0-8ed0afad4617","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import count, col\nfrom pyspark.sql.types import StructType, StructField, StringType\n\n# Define the schema for the JSON data\njson_schema = StructType([\n  StructField(\"timestamp\", StringType(), True),\n  StructField(\"ip_address\", StringType(), True),\n  StructField(\"user_agent\", StringType(), True)\n])\n\n# Read the JSON data into a DataFrame\njson_data = spark.read.schema(json_schema).json(\"dbfs:/FileStore/tmp/u.item.csv\")\n\n# Group the data by IP address and count the occurrences\nip_counts = json_data.groupBy(\"ip_address\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc())\n\n# Show the top 10 IP addresses by count\nip_counts.show(10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a891e47f-5a32-4c04-a94d-96286c337d1e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+\n|ip_address|count|\n+----------+-----+\n|      null| 1682|\n+----------+-----+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"prerna","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":295471767776674,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":295471767776673}},"nbformat":4,"nbformat_minor":0}
