from pyspark.sql import SparkSession


spark = SparkSession.builder.appName("WordCount").getOrCreate()


lines = spark.read.text("input_file.txt").rdd.map(lambda r: r[0])

# Split lines into words and count frequency of each word
counts = lines.flatMap(lambda line: line.split(" ")) \
              .map(lambda word: (word, 1)) \
              .reduceByKey(lambda a, b: a + b)


counts.saveAsTextFile("output_dir")


spark.stop()
